\subsection{Signaling Protocol}

Our signaling protocol consists in send and receiving JSON formated messages over WebSockets by both the server and client. 

\begin{lstlisting}[caption={General structure of our WebSocket messages},language=json]
{
	"cmd":<cmd>,
	"data":<data>
}
\end{lstlisting}

When a user enters in a group conference, after the page is completely loaded it is created a WebSocket that mantains a connection with our web servers. 
Before creating the web socket, we must identify the user and check if he has permissions to participate in the conference. This is done by retrieving the session id from the cookie provided by the user through the \ac{HTTP} headers. It's important to save de user identification before the socket is created because after the the handshake performed by the \emph{WebSocket} protocol\cite{rfc6455} the \ac{HTTP} context is lost.

When the connection is established between the server and the client, a \emph{PeerConnection} is created on the client and imediately after an \ac{WebRTC} endpoint on the server specifying a possible set of \ac{ICE} servers to connect.

The user is asked if he whants to share its camera and microphone, share screen or just receive streams from the server. If the user decides to perform a screen share, \emph{adapter.js}\footnote{\url{https://github.com/Temasys/AdapterJS}(accessed March 15, 2015).} may ask to install a pluggin if the browser doesn't support screen sharing.

If the user decides to share either from camera of screen, \emph{getUserMedia} is called with the correspondent contraints in order to obtain a local stream. 

\begin{lstlisting}[caption={Media constraints},language=JavaScript]
var screenShareConstraints = {	
	"video": {
		"mediaSource": "window" || "screen"
	}, 
	"audio": false
};
var cameraMicrophoneConstraints = {
	"audio":true, 
	"video":true 
};
var receiveOnlyConstraints = {
	"offerToReceiveAudio":true,
	"offerToReceiveVideo":true
};
\end{lstlisting}

A popup is raised in order to ask the user to give permission to share those resources. If the resources are shared successfully the user creates an offer, sets a \emph{local session description} to its \emph{PeerConnection} and sends it through WebSockets. If the resources are not shared or the user specified to receive stream only, an offer is created specifying the contraints for receiving only video and audio.

The server receive and processes the offer and sets the \emph{remote session description} to its client associated \ac{WebRTC} endpoint. Then a \emph{local session description} is created on the server and sent back to the client, after that the server trys to gather \ac{ICE} candidates.

The client receives the server answer, sets the \emph{remote session description} and gets the ice candidates from the \ac{ICE} server.

After a while both the server and client receive the \ac{ICE} candidates that allow the client to connect directly to \ac{KMS} and vice-versa. The candidates are received at the client which sets them to its \emph{PeerConnection}. The same is done on the server which receives the \ac{ICE} candidates from the client and propagates them to \ac{KMS}.

The media session is established, the server starts to record any received stream and the client created an \ac{URL} correspondent to the stream location.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
    	\includegraphics[width=\textwidth]{figures/signaling}
    \end{subfigure}
    \caption{Signaling sequence diagram}
\end{figure} 
