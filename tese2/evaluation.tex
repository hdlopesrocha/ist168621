\chapter{Evaluation}
\label{chapter:evaluation}
  In the previous chapter we described how we implemented our solution namely the data model that we used, the description of the signaling protocol, how we performed stream recording, how we overlayed interactive content to video and lastly how we deployed our solution. 

  In this chapter we describe how we tested our solution, show and analyze the results in order to validate the contributions of this thesis.

\section{Tests Objectives}

  We have tested our solution with real users for a better understanding of their difficulties and what can be done in order to improve our solution's usability.

  We have also tested the performance of our solution by measuring the used resources. Those performance tests are crucial to ensure that our solution is in fact stable and users can use it timelessly without decreasing the quality of their experience. 


  \section {Performance Tests}

     In this section we describe performance test scenarios that we have applied and their respective results.


    \subsection{Tests Scenarios}


      In order to benchmark our system, we have implemented a small \emph{Python} script using \emph{psutil}\footnote{\url{https://github.com/giampaolo/psutil} (Accessed March 27, 2016)} that collects with a periodicity of one second; CPU, physical memory information relative to each running process and network usage relative to each interface. 

      We would like to collect network information relative to each process, which \emph{nethogs}\footnote{\url{https://raboof.github.io/nethogs/} (Accessed March 27, 2016)} provide but it could not capture the network usage of some processes. Although this would be useful for a deep analysis, we know which network interfaces are used to establish connections between processes as we show on table \ref{table:interfacemap}.

  \begin{table}[!htb]
\centering
\caption{Mapping of connections by network interface}
\label{table:interfacemap}
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Interface}} & \multicolumn{1}{c|}{\textbf{Connection}}         \\ \hline
Loopback & Web Server $\leftrightarrow$ MongoDB  \\ \hline
Loopback & Web Server $\leftrightarrow$ Kurento Media Server  \\ \hline
Loopback & Kurento Media Server $\leftrightarrow$ Kurento Repository $\leftrightarrow$ MongoDB  \\ \hline
Ethernet & Web Server $\leftrightarrow$ Client\\ \hline
Ethernet & Kurento Media Server $\leftrightarrow$ Client\\ \hline
\end{tabular}
\end{table}


      The performance test scenario that we have defined consists on two phases, the first phase consists only on having users, with similar computer and network specifications, entering sequentially on the conference room, the second phase consists on the users leaving the conference room. Each event, joining and leaving, occurs with intervals of one minute in total of thirteen minutes (780 seconds).

  


\subsection{Test Results}
  
      We are presenting in this section the results of our performance tests after implementing our solution.
\subsubsection{Network usage}

      Every time a user shares its camera or its screen in the context of a conference room, its offered video is sent to the server and independently from watching an individual stream or the mixed version the server just sends a unique stream back to the user.

      From the media server prespective if there are $n$ clients connected each of them sending and receiving one stream, it is expected that server sends and receives also $n$ streams. By so we expect that the amount of network increases linearly as users join a conference room.

      Figure \ref{fig:test_full_features_net} confirms our expectations, each vertical yellow line represents each event, the first seven events are users entering the conference room the next ones represent users leaving the conversation. 
      

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{stats/test_full_features_net.eps}
  \caption{Network usage after implementing all features}
  \label{fig:test_full_features_net}
\end{figure}


      The blue peaks are caused by the signaling phase and web page downloads, including resources such as images, stylesheets and javascript files. 
      The green peaks are caused by that being transfered between \ac{KMS} and \emph{MongoDB} through \emph{Kurento Repository} each peak occurs each time a block of video is recorded which in this case is every ten seconds. 
      The recordings are synchronized so all user and mixed blocks starts and ends at the same time, that's why the amount of work done every ten seconds accumulates and because this is performed locally the maximum transfer rate is limited by the performance of the memory as buffers are written to buffers then to disks. 
      Client's sent data transfer rate has no significant peaks as signaling information contains few information.

      Figure \ref{fig:summary_full_net} shows the average transmission rate per interval of consecutive events. 


\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{stats/summary_full_net.eps}
  \caption{Average network usage per interval of events after implementing all features}
  \label{fig:summary_full_net}
\end{figure}



If we consider all streams equal and an incoming stream uses $x$kbps, with $n$ incoming streams the rate of data received at \ac{KMS} is $nx$ and the expected rate of data transfered to \emph{Kurento Repo} using localhost is $(n+1)x$. Due to the data being transfered also from \emph{Kurento Repo} to \emph{MongoDB} via localhost, the expected total rate of data transfered through localhost is $2((n+1)x) = (2n+2)x$, which explains the reason for the average amount of data transfered through localhost being more than twice the average amount of data received from users.

With this results we conclude that if we want to scale our solution's storage using the \emph{MongoDB}'s cluster configuration, both \emph{Kurento Repository} and \emph{MongoDB} should be installed in the same machine because the loopback interface can handle bigger transfer rates than the remaining network interfaces. For the same reason another possible configuration for scaling the storage is to install both \emph{Kurento Repository} and \ac{KMS} on the same machine.

On the other hand figure \ref{fig:test_client_net} shows the respective network usage on the client side during our test case. We observe that int he first seconds the client adjusts the video quality that sends to \ac{KMS}, whenever a new client enters in the conference we observer that \ac{KMS} decreases the video quality in order to instantaneously integrate a new user into the conference room, after a while \ac{KMS} realizes that its network can handle the increase of clients and sends the video with a better quality to every participant. When a user leaves the conference room \ac{KMS} has no need to decrease the participant's video quality as less network bandwidth will be used.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{stats/test_client_net.eps}
  \caption{Client Network usage during our test case}
  \label{fig:test_client_net}
\end{figure}



\subsubsection{Memory usage}


Figure \ref{fig:test_full_features_mem} shows the memory usage during our performance test case. Both \ac{JVM}, \emph{MongoDB} and \ac{KMS} performs their own memory management by holding and recycling objects when needed. The expected and observed behavior of the memory usage is growth memory usage while the users are entering in the conference room and a memory usage stabilization afterwards.



\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{stats/test_full_features_mem.eps}
  \caption{Memory usage after implementing all features}
  \label{fig:test_full_features_mem}
\end{figure}


\emph{MongoDB} memory usage keeps increasing because it tries to fit part of the database on \ac{RAM} for fast read access. \emph{MongoDB} checkpoints data to disk every 60 seconds or when journal data exceeds 2GB\footnote{\url{https://docs.mongodb.org/manual/faq/storage/}(Accessed March 28, 2016)}, that explains the small memory usage peaks during our test case. When the conference room is empty no there are no video recordings, which explains the memory stabilization at the end.

\emph{KMS} memory usage was increasing and in fact we did not expected that, we have done more tests and we have observed that by disabling the recorder feature memory usage kept linearly related to the amount of clients present on the room. Therefore, we have verified our implementation and we have concluded that nothing was wrong with our code, we have confirmed that every allocated resource was indeed released after the end of recording. As a result of our intensive tests and consecutive failures we suspected that the problem was not ours.

Hence, in order to confirm our suspicions, we have decided to read the implementation of \ac{KMS} and we have found that \ac{KMS} was not releasing the memory if the recorder was stopped by us. Subsequently, we have changed the source code and submitted to \emph{Kurento} team through a \emph{GIT}'s \emph{"pull request"}.


Figure \ref{fig:test_ram_fixed_mem} shows the memory usage during our performance test case taking into account the patch that we applied to \ac{KMS}.



\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{stats/test_ram_fixed_mem.eps}
  \caption{Memory usage after fixing recorder memory leak}
  \label{fig:test_ram_fixed_mem}
\end{figure}


\subsubsection{CPU usage}


Figure \ref{fig:test_full_features_cpu} shows the percentage of \ac{CPU} usage during our performance test case. Each 100\% represents one \ac{CPU} core, although that does not mean one \ac{CPU} is fully used, for example two cores at 60\% represent 120\% \ac{CPU} usage. As we can see, the percentage of \ac{CPU} used increases and decreases linearly in function of the amount of conference participants. 

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{stats/test_full_features_cpu.eps}
  \caption{Percentage of CPU used during the performance tests}
  \label{fig:test_full_features_cpu}
\end{figure}

Figure \ref{fig:test_full_features_cpu_zoom} is the same as figure \ref{fig:test_full_features_cpu} but zoomed over \emph{MongoDB}, \emph{Java} and \emph{Kurento Repository}. As we can see there are periodic processing peaks every ten seconds that are due to block recording. 

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{stats/test_full_features_cpu_zoom.eps}
  \caption{Percentage of CPU used during the performance tests (zoomed)}
  \label{fig:test_full_features_cpu_zoom}
\end{figure}


   Figure \ref{fig:summary_full_cpu} shows the average used \ac{CPU} per interval of consecutive events. 

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{stats/summary_full_cpu.eps}
  \caption{Average percentage of CPU used per interval of events after implementing all features}
  \label{fig:summary_full_cpu}
\end{figure}



  Just for testing purposes we have done the same performance tests without detecting \emph{QR codes} in order to understand how \ac{CPU} intensive is this task. Figure \ref{fig:test_without_qrcode_cpu} shows the results for the same experience but with \emph{QR code} detection disabled.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{stats/test_without_qrcode_cpu.eps}
  \caption{Percentage of CPU used during the performance tests without QR code detection}
  \label{fig:test_without_qrcode_cpu}
\end{figure}

We conclude that \emph{QR code} detection is a very intensive task, approximately doubling the amount of work performed by \ac{CPU}. Without this feature the network and memory usage had insignificant changes compared to \ac{CPU} usage.

\subsubsection{Two consecutive test cases}


We have also performed two consecutive test cases in order to understand if we the influence of the first test case over the second, as observed in figure \ref{fig:test_two_times_cpu}, there is no \ac{CPU} influence between two test cases.


\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{stats/test_two_times_cpu.eps}
  \caption{Percentage of CPU used during two consecutive test cases}
  \label{fig:test_two_times_cpu}
\end{figure}

On the other hand due to memory recycling techniques we can observe in figure \ref{fig:test_two_times_mem} that some of the memory that was allocated in the first test was reused in the second case.


\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{stats/test_two_times_mem.eps}
  \caption{Memory usage during two consecutive test cases}
  \label{fig:test_two_times_mem}
\end{figure}


\subsubsection{One hour test case}


   Figure \ref{fig:test_hour_mem} shows the memory used by our solution with seven clients simultaneously on the same conference room during one hour. As we can observe \ac{KMS} memory usage stabilizes.


\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{stats/test_hour_mem.eps}
  \caption{Memory usage during one hour with seven clients}
  \label{fig:test_hour_mem}
\end{figure}





\section {Usability Tests}
     In this section we describe usability test scenarios that we have applied and their respective results.


    \subsection{Tests Scenarios}

      In order to the usability of our solution, we haver performed usability tests with the help of real users with different backgrounds and ages.

      We handled a guide to the users with five tasks to perform, the metrics we used for each task were: number of clicks, number of errors including a description and time spent. 

      The tasks we asked to the user are:

      \begin{enumerate}
      \item Login into the system with the provided credentials and accept the received friendship request. Find the coordinator's brother and add him as a friend.

      \item Create a private conference room, enter and share your screen, add the coordinator to the conference room and chat with him. After that use the collaborative editor in order to write at the same time as the coordinator. Lastly save the editor and leave the conference room.

      \item Enter in the conference room correspondent to the third task as an observer and navigate to the specified annotation. Watch the video until a list of topics is overlayed in the video and choose one of them and leave the conference room.

      \item Enter in the conference room correspondent to the fourth task by sharing your camera and create a time annotation in the instant of time when you entered, navigate to the current time, search for the annotation and leave the conference room.

      \item Enter in the conference room correspondent to the fourth task by sharing your camera and add a subtitle in the video, preview it, specify an interval of time and save it. Then show the provided \emph{QR code} to the camera and leave the leave the conference room.
      \end{enumerate}

      The goal of the first task consists on evaluating the interface for authentication and friendship management.

      The second task is used to make the users familiar with our tools for collaborative content edition.

      The goal of the third task is to demonstrate the navigation functionalities and synchronized interactive content superposition.

      The fourth task is used to analyse the behavior and difficulties of the user when creating a time annotation.

      The goal of the fifth task is to create synchronized overlayed content, demonstrate the difficulties of such task and present an easier way to synchronize content.

  \subsection{Test Results}

In this section we present the results of our usability tests. The first time we tested our solution by providing tasks to users, we have observed that our solution was not perfect. We have faced usability problems during our tests, he had to improve our solution's usability and start the tests again.

  \subsubsection {First phase}

In the first testing phase we have performed just three tests with users and ceased for improvements. 

Both users that tested our solution had difficulties on the first task when finding new friendship requests and adding new friends, as friendship request were possible to access only from the top bar and new user profiles were only accessible through the search bar. In order to solve this problem we have added a menu for adding new users in the friends list and another near the friends list as it can be seen on figure \ref{fig:test_ui_01_02_03}.

\begin{figure}[!htb]
\centering
\begin{minipage}[b]{0.2\linewidth}
\centering

    \includegraphics[width=\textwidth]{figures/test_ui_01.png}
        a) Friend list path
\label{fig:minipage1}
\end{minipage}
\begin{minipage}[b]{0.35\linewidth}
    \centering

    \includegraphics[width=\textwidth]{figures/test_ui_02.png}
         b) Friend requests path
\label{fig:minipage2}
\end{minipage}
\begin{minipage}[b]{0.2\linewidth}
    \centering

    \includegraphics[width=\textwidth]{figures/test_ui_03.png}
         c) Search bar path
\label{fig:minipage2}
\end{minipage}

    \caption{Multiple paths for finding people}
    \label{fig:test_ui_01_02_03}
\end{figure}

On the second task one user clicked on the friends list for adding a friend to room and by consequence made him leave the conference room to the user profile. We solved this problem by showing a pop-up with the user profile and an additional button to add the user to the conference room as it can be seen on figure \ref{fig:test_ui_04}.

\begin{figure}[!htb]
\centering
\begin{minipage}[b]{0.7\linewidth}
\centering

    \includegraphics[width=\textwidth]{figures/test_ui_04.png}
\end{minipage}


    \caption{Adding user to conference room from user profile}
    \label{fig:test_ui_04}
\end{figure}

On the third task we noticed that both users were not expecting to search the content directly from the search bar, instead they used the time-line.

On the fourth task both users didn't placed their annotations on right place, we only provide one way to place annotations, which is by dragging them on the time-line. 
After creating an annotation users also didn't understand that they had to save their changes on the time-line, we solved this problem by saving automatically each time the user changed any annotation, the old user interface can be seen on figure \ref{fig:test_ui_05}.

\begin{figure}[!htb]
\centering
\begin{minipage}[b]{0.3\linewidth}
\centering

    \includegraphics[width=\textwidth]{figures/test_ui_05.png}
\end{minipage}


    \caption{Save button that was removed}
    \label{fig:test_ui_05}
\end{figure}

Both users had success with fifth task but they revealed difficulties choosing the time interval for the hyper content, we are aware that this is a difficult task and that's why we have introduced the \emph{QR code} content creation mechanism which was very easy to learn.

We have also gathered comments and suggestions after letting the users explore our system, extra improvements were made such as:

\begin{itemize}
\item Giving feedback to user when saving content and collaborative editors.
\item Support file upload on our chat.
\item Starting with our time-line more zoomed in order to give more sensation of time.
\item Create content with starting time and duration instead ending time.
\end{itemize}

  \subsubsection {Second phase}

On the second phase of our user interface tests, in a general view, we have noticed great improvements on the learning time.

In order to measure the users learning speed, we have performed tests with experienced users in order to retrieve the optimal task duration and minimal task clicks.

To this end, with regard to optimal task duration and minimal clicks, we obtained the values shown in table \ref{table:optimal}.


\begin{table}[H]
\centering
\caption{Metrics for an experienced user}
\label{table:optimal}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Task} & 1 & 2 & 3 & 4 & 5 \\ \hline
\textbf{Duration (seconds)} & 20 & 35 & 30 & 25 & 35 \\ \hline
\textbf{Number of clicks} & 7 & 10 & 5 & 9 & 7 \\ \hline
\end{tabular}
\end{table}


From the data that we have collected, namely the task duration (figure \ref{fig:user_times}), number of clicks (figure \ref{fig:user_clicks}), number of errors (figure \ref{fig:user_errors}), task difficulty (figure \ref{fig:user_diffs}) and qualitative evaluation (figure \ref{fig:user_evals}), we have calculated the confidence intervals in order to understand the most plausible values for each metric.

As a result of both true average and variance being unknown and the usage of a relatively small amount of samples, we had to use \emph{t-distribution} to estimate our metrics confidence intervals. 

Accordingly, we have used the following formula to calculate the intervals for the expected value of the true average with 95\% confidence:
\begin{equation}
[\bar{x}-F^{-1}_{t_{(n-1)}}(1-\alpha/2)\times \frac{s}{\sqrt{n}}, \bar{x}+F^{-1}_{t_{(n-1)}}(1-\alpha/2)\times \frac{s}{\sqrt{n}}]
\end{equation}

Where $\alpha=0.05$, $n$ is the number of samples (which may not coincide with the number of tests due to failed tasks), $\bar{x}$ is the sample mean, $x_i$ is the sample value and $s$ is the sample standard deviation that is given by:
\begin{equation}
s=\sqrt{\frac{1}{n-1}\Sigma^{n}_{i=1}(x_i-\bar{x})^2}
\end{equation}


\begin{figure}[!htb]
\centering
\begin{minipage}{.5\textwidth}
  \centering
    \includegraphics[width=\textwidth]{stats/user_times.eps}
    \caption{Time spent per task}
    \label{fig:user_times}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
    \includegraphics[width=\textwidth]{stats/user_clicks.eps}
    \caption{Mouse clicks per task}
    \label{fig:user_clicks}
\end{minipage}
\end{figure}



Accordingly to figure \ref{fig:user_evals}, we can observe that most users had less difficulties with the first two tasks, which represents types of tasks that most users are familiar with. As soon as users had to navigate in time, manipulate annotations and create content (respectively \emph{task3}, \emph{task4} and \emph{task5}) we have observed that they revealed more difficulties. Most of those difficulties, based on the users feedback, were mainly due to those concepts not being familiar to them.

\begin{figure}[!htb]
\centering
\begin{minipage}{.5\textwidth}
  \centering
    \includegraphics[width=\textwidth]{stats/user_errors.eps}
  \caption{Errors per task}
  \label{fig:user_errors}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
    \includegraphics[width=\textwidth]{stats/user_diffs.eps}
  \caption{Difficulty per task}
  \label{fig:user_diffs}
\end{minipage}
\end{figure}


Although we had relatively bad results with some users, we have explained to the users that could not conclude the tasks or performed incorrectly the most efficient way to perform the requested tasks. Some users suggested to display more hints in order to achieve a faster learning but after all they were impressed and gave us a better evaluation (as seen on figure \ref{fig:user_evals}).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{stats/user_evals.eps}
  \caption{Solution evaluation}
  \label{fig:user_evals}
\end{figure}



Most users gave us worse evaluations on our user interface layout and content editor, which was due to having a lot of tools present in the same web page and some of them were hidden due the screen size. In some cases users had to scroll down in order to find the tools they were looking for. 

Another weak aspect was our content editor, which in fact we recognize the difficulty to handle with, most due to the amount of information that is necessary to create a synchronized content (starting time, duration and content itself). Some users have suggested that the content should also be present on the time-line so they could be easily dragged and resized (on time).

We are aware that placing content on the time-line will reduce our solutions performance, especially when there is a relatively big amount of content due to the content that is present on the time-line being loaded all in once. Although, we recognize that for some cases (relatively low amount of content), displaying the content on the time-line could not have a great impact on our solutions performance. 

In conclusion, 100\% of our testers though that our solution was an innovation and 92.31\% recommended using our solution.





